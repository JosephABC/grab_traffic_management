{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SBSCBPi-yvq"
   },
   "source": [
    "This Notebook is the code which was used to generate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KX7r4GPNMD4z"
   },
   "source": [
    "\n",
    "Link to Google Drive where the data file has been placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlMYVIzNLDNG"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UvP4AZaxMIWV"
   },
   "source": [
    "Use the Pandas Library to read the data csv file. And Install Geohash2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cxcEo8PgLhzM"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4e7928f25923>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#df = pd.read_csv(\"gdrive/My Drive/GrabAI/training.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./training.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install git+https://github.com/dbarthe/geohash/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgeohash2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"gdrive/My Drive/GrabAI/testing.csv\")\n",
    "!pip install git+https://github.com/dbarthe/geohash/\n",
    "import geohash2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K6ej4qzXL5Ts"
   },
   "source": [
    "Sort Dataframe by Day and TimeStamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5QSdVGHLphM"
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['day','timestamp'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dJ8N0GFcLy_z"
   },
   "source": [
    "Calculate Time in mins. A random id is assigned to each fragment for progress tracking.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xdKgqYsXLs-j"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "import numpy as np\n",
    "from multiprocessing import Pool\n",
    "import random\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "  fragment_array = np.array_split(df, 20)\n",
    "  pool = Pool(20)\n",
    "  df = pd.concat(pool.map(func, fragment_array))\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  return df\n",
    "\n",
    "def get_time(x):\n",
    "  hour, minute = x[1].split(':')\n",
    "  return int(x[0]-1)*24*60 + int(hour)*60 + int(minute)\n",
    "\n",
    "def test_func(data):\n",
    "  fragment_id = random.randint(0,999)\n",
    "  print('Running: {}'.format(fragment_id))\n",
    "  data['time'] = data[['day','timestamp']].apply(get_time, axis=1)\n",
    "  print('Done: {}'.format(fragment_id))\n",
    "  return data\n",
    "\n",
    "df = parallelize_dataframe(df, test_func)\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6pQpuqeqMNmm"
   },
   "source": [
    "Write  df to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gtqcEdyxLwcM"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = open(\"df.pickle\",\"wb\")\n",
    "pickle.dump(df, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZXhz-kk2MP1s"
   },
   "source": [
    "Retrieve df from Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMStxHfnMT8s"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "infile = open(\"df.pickle\",\"rb\")\n",
    "df = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "50Q9cknOnQtv"
   },
   "source": [
    "Prepare Main Time Series Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3I_F3-bUnPdU"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, Manager\n",
    "import random\n",
    "\n",
    "unique_geohash_list = list(np.unique(df['geohash6']))\n",
    "n_unique_geohash = len(unique_geohash_list)\n",
    "geohash_dict = {unique_geohash_list[i]:i for i in range(n_unique_geohash)}\n",
    "max_time = np.amax(df['time'])\n",
    "time_list = list(range(0, max_time, 15))\n",
    "demand_df = pd.DataFrame(time_list, columns=['time'])\n",
    "processes = 40\n",
    "\n",
    "def parallelize_dataframe(df, func):\n",
    "  fragment_array = np.array_split(df, processes)\n",
    "  pool = Pool(processes)\n",
    "  df = pd.concat(pool.map(func, fragment_array))\n",
    "  pool.close()\n",
    "  pool.join()\n",
    "  return df\n",
    "\n",
    "def get_demand_snapshot(x):\n",
    "  demand_snapshot = np.zeros(n_unique_geohash)\n",
    "  time_df = df['time'] == x\n",
    "  df_snapshot = df[time_df]\n",
    "  for _, row in df_snapshot.iterrows():\n",
    "    demand_snapshot[geohash_dict[row['geohash6']]] = row['demand']\n",
    "  return demand_snapshot\n",
    "\n",
    "def test_func(data):\n",
    "  fragment_id = random.randint(0,999)\n",
    "  print('Running: {}'.format(fragment_id))\n",
    "  data['demand_snapshot'] = data['time'].apply(get_demand_snapshot)\n",
    "  n_done[0] += 1\n",
    "  print('Done: {}, {}/{}'.format(fragment_id, n_done[0], processes))\n",
    "  return data\n",
    "\n",
    "n_done = Manager().list(range(1))\n",
    "n_done[0] = 0\n",
    "demand_df = parallelize_dataframe(demand_df, test_func)\n",
    "toc = time.time()\n",
    "print(toc - tic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7pq8Hy_gsxK2"
   },
   "source": [
    "Restart package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IlPKpX7neb7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "infile = open(\"df.pickle\",\"rb\")\n",
    "df = pickle.load(infile)\n",
    "infile = open(\"demand_df.pickle\",\"rb\")\n",
    "demand_df = pickle.load(infile)\n",
    "infile.close()\n",
    "infile.close()\n",
    "unique_geohash_list = list(np.unique(df['geohash6']))\n",
    "n_unique_geohash = len(unique_geohash_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EUI8cMlN_-33"
   },
   "source": [
    "Write  demand_df to Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q9tt6udD_xbR"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "outfile = open(\"demand_df.pickle\",\"wb\")\n",
    "pickle.dump(demand_df, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2Q-ya_TAAjm"
   },
   "source": [
    "Retrieve  demand_df from Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sDVPCi2S_2Dw"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "infile = open(\"demand_df.pickle\",\"rb\")\n",
    "demand_df = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "veWQ6reDTqKl"
   },
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8v1VteA6_RR"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "tic = time.time()\n",
    "\n",
    "total_time = 30\n",
    "training_df = demand_df['demand_snapshot'][demand_df['time'] <= (total_time)*24*60]\n",
    "\n",
    "data_list= training_df.values.tolist()\n",
    "data_arr= np.empty([len(data_list),0])\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "  n_vars = n_unique_geohash\n",
    "  df = pd.DataFrame(data)\n",
    "  global data_arr\n",
    "\n",
    "  # input sequence (t-n, ... t-1)\n",
    "  for i in range(n_in, 0, -1):\n",
    "    data_arr = np.append(data_arr, df.shift(i).values, axis=1)\n",
    "    print('Input Sequence {}/{} Left'.format(i,n_in))\n",
    "  \n",
    "  # forecast sequence (t, t+1, ... t+n)\n",
    "  for i in range(0, n_out):\n",
    "    data_arr = np.append(data_arr, df.shift(-i).values, axis=1)\n",
    "\n",
    "  print('Putting it together...')\n",
    "  # drop rows with NaN values\n",
    "  data_arr = data_arr[~np.isnan(data_arr).any(axis=1)]\n",
    "    \n",
    "  return\n",
    "print('Working on Forecast Sequence...')\n",
    "series_to_supervised(data_list, 96, 6)\n",
    "print(data_arr.shape)\n",
    "\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z878pUNsTs94"
   },
   "source": [
    "Training and Testing Data Set Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cUXBj94FB0Jw"
   },
   "outputs": [],
   "source": [
    "training_days = 25\n",
    "values = data_arr\n",
    "n_features = n_unique_geohash\n",
    "n_time_step = 96\n",
    "n_obs = int((n_time_step+1) * n_features)\n",
    "n_total_time = training_days * 24 * 4\n",
    "train = values[:n_total_time, :]\n",
    "test = values[n_total_time:, :]\n",
    "print(train.shape, test.shape)\n",
    "train_X, train_y = train[:, :n_obs], train[:, n_obs:n_obs + n_features*5]\n",
    "test_X, test_y = test[:, :n_obs], test[:, n_obs:n_obs + n_features*5]\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "train_X = train_X.reshape((train_X.shape[0], n_time_step+1, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_time_step+1, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ua6ACQznTxdS"
   },
   "source": [
    "Recurrent Neural Network Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0D-WphB5Wytn"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(LSTM(256, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(6645,activation=tf.nn.relu))\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "# fit network\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=256, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U27TuwyNiKm0"
   },
   "source": [
    "Get T+1 to T+5 values and place in DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_YpdmhF30v8H"
   },
   "outputs": [],
   "source": [
    "yhat = model.predict(test_X)\n",
    "names = list()\n",
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['geohash'] = unique_geohash_list\n",
    "for i in range(0,5):\n",
    "  start = n_unique_geohash*i\n",
    "  end = (i+1)*n_unique_geohash\n",
    "  demand = yhat[0][start:end]\n",
    "  predictions_df['T + {}'.format(i+1)] = demand\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ToP1VFh6myld"
   },
   "source": [
    "Calculate RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "id": "JNDCZSXhYkps",
    "outputId": "65d30fc4-3525-4b50-c2d5-198dc467b7ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for \n",
      "T+15: 0.017088448323991992\n",
      "T+30: 0.020636314772696267\n",
      "T+45: 0.01905477288018001\n",
      "T+60: 0.019645188305588854\n",
      "T+75: 0.022823923811942613\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import sqrt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rmse = np.zeros(5)\n",
    "# calculate RMSE\n",
    "for i in range(0,5):\n",
    "  start = n_unique_geohash*i\n",
    "  end = (i+1)*n_unique_geohash\n",
    "  rmse[i] = sqrt(mean_squared_error(test_y[0,start:end], yhat[0,start:end]))\n",
    "\n",
    "print('RMSE for \\nT+15: {}\\nT+30: {}\\nT+45: {}\\nT+60: {}\\nT+75: {}'.format(*rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OR9zcLev0_NC"
   },
   "outputs": [],
   "source": [
    "model.save(\"gdrive/My Drive/GrabAI/model_30_96_25.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Grab_TrafficManagement_Submission.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
